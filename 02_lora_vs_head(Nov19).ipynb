{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a2cf1c-6e63-4f1b-a6e4-3e45bffcc8a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers>=4.46 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (4.57.0)\n",
      "Collecting transformers>=4.46\n",
      "  Using cached transformers-4.57.1-py3-none-any.whl.metadata (43 kB)\n",
      "Requirement already satisfied: datasets>=2.19 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (4.2.0)\n",
      "Collecting datasets>=2.19\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: peft in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (0.17.1)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: safetensors in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (0.6.2)\n",
      "Requirement already satisfied: filelock in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (0.35.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (2024.9.11)\n",
      "Requirement already satisfied: requests in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (0.22.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from transformers>=4.46) (4.66.5)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.19) (21.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.19) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.19) (2.2.3)\n",
      "Requirement already satisfied: httpx<1.0.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.19) (0.27.0)\n",
      "Requirement already satisfied: xxhash in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.19) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.19 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from datasets>=2.19) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (2024.6.1)\n",
      "Requirement already satisfied: psutil in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from peft) (2.8.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from peft) (1.10.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (3.10.5)\n",
      "Requirement already satisfied: anyio in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.19) (4.2.0)\n",
      "Requirement already satisfied: certifi in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.19) (2025.7.14)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.19) (1.0.2)\n",
      "Requirement already satisfied: idna in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.19) (3.7)\n",
      "Requirement already satisfied: sniffio in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from httpx<1.0.0->datasets>=2.19) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.19) (0.14.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46) (4.11.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=4.46) (1.1.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.46) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from requests->transformers>=4.46) (2.2.3)\n",
      "Requirement already satisfied: setuptools in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.1.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.3)\n",
      "Requirement already satisfied: jinja2 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.19) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.19) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets>=2.19) (2023.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (2.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets>=2.19) (1.11.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets>=2.19) (1.16.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/mh/opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Using cached transformers-4.57.1-py3-none-any.whl (12.0 MB)\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: transformers, datasets, peft\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.57.0\n",
      "    Uninstalling transformers-4.57.0:\n",
      "      Successfully uninstalled transformers-4.57.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 4.2.0\n",
      "    Uninstalling datasets-4.2.0:\n",
      "      Successfully uninstalled datasets-4.2.0\n",
      "  Attempting uninstall: peft\n",
      "    Found existing installation: peft 0.17.1\n",
      "    Uninstalling peft-0.17.1:\n",
      "      Successfully uninstalled peft-0.17.1\n",
      "Successfully installed datasets-4.4.1 peft-0.18.0 transformers-4.57.1\n"
     ]
    }
   ],
   "source": [
    "!pip install -U \"transformers>=4.46\" \"datasets>=2.19\" peft safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4be102e9-961a-4824-ab42-5203640b5fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: mps\n"
     ]
    }
   ],
   "source": [
    "#B) ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "import os, random, time, csv\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "#ðŸŸ¢ ã€Œãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ ï¼ž ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã€\n",
    "#ðŸŸ¢ ãã‚Œã‚’ã¾ã¨ã‚ã¦ pip install ã§å…¥ã‚Œã‚‹â€œè£½å“ä¸€å¼â€ãŒãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "\n",
    "SEED = 42\n",
    "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
    "\n",
    "DEVICE = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "print(\"Device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "829e0e49-427c-468e-9d55-48185f581ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train/hold: 1000 200\n"
     ]
    }
   ],
   "source": [
    "#C) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆé¸æŠž & æ­£è¦åŒ–ï¼ˆå®Ÿå‹™å¯„ã‚Šï¼‰\n",
    "# \"alpaca\" | \"dolly\" | \"squad\" ã‹ã‚‰é¸æŠž\n",
    "DATASET = \"alpaca\"\n",
    "MAX_ROWS = 1200\n",
    "HOLDOUT  = 200\n",
    "\n",
    "def load_and_map(name):\n",
    "    rows=[]\n",
    "    if name==\"dolly\":\n",
    "        ds = load_dataset(\"databricks/databricks-dolly-15k\", split=\"train\")\n",
    "        for r in ds:\n",
    "            inst = (r.get(\"instruction\") or \"\").strip()\n",
    "            ctx  = (r.get(\"context\") or r.get(\"input\") or \"\").strip()\n",
    "            out  = (r.get(\"response\") or r.get(\"output\") or \"\").strip()\n",
    "            if not inst or not out: continue\n",
    "            if ctx: inst = inst + \"\\n\" + ctx\n",
    "            rows.append({\"instruction\":inst, \"output\":out})\n",
    "    elif name==\"alpaca\":\n",
    "        ds = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "        for r in ds:\n",
    "            inst = r[\"instruction\"].strip()\n",
    "            inp  = (r.get(\"input\") or \"\").strip()\n",
    "            out  = r[\"output\"].strip()\n",
    "            if inp: inst = inst + \"\\n\" + inp\n",
    "            rows.append({\"instruction\":inst, \"output\":out})\n",
    "    elif name==\"squad\":\n",
    "        ds = load_dataset(\"squad_v2\", split=\"train\")\n",
    "        for r in ds:\n",
    "            ctx = r[\"context\"].strip(); q = r[\"question\"].strip()\n",
    "            answers = r[\"answers\"][\"text\"]\n",
    "            if answers:\n",
    "                inst = f\"Answer based on the context.\\nContext: {ctx}\\nQuestion: {q}\"\n",
    "                out  = answers[0].strip()\n",
    "                rows.append({\"instruction\":inst, \"output\":out})\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "    rows = rows[:MAX_ROWS]\n",
    "    random.shuffle(rows)\n",
    "    return rows\n",
    "\n",
    "rows = load_and_map(DATASET)\n",
    "train = rows[:-HOLDOUT]\n",
    "hold  = rows[-HOLDOUT:]\n",
    "print(\"train/hold:\", len(train), len(hold))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7dc6a0-b9cf-4f38-9e55-875a9e21eaf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 sample lens: 256\n"
     ]
    }
   ],
   "source": [
    "#D) ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºï¼ˆ-100 ãƒžã‚¹ã‚¯ï¼‰\n",
    "MODEL_NAME = \"distilgpt2\"   # MPSã§ã‚‚è»½é‡ã§å®‰å®š\n",
    "BLOCK = 256                  # é‡ã‘ã‚Œã° 192 ã¸\n",
    "BATCH = 8                    # é‡ã‘ã‚Œã° 4 ã¸\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "if tok.pad_token is None:\n",
    "    tok.pad_token = tok.eos_token\n",
    "\n",
    "def apply_template(instruction, answer=\"\"):\n",
    "    prompt = f\"### Instruction:\\n{instruction}\\n### Response:\\n\"\n",
    "    return prompt, f\"{answer}{tok.eos_token}\"\n",
    "\n",
    "def encode_row_fixed(row):\n",
    "    prompt, answer = apply_template(row[\"instruction\"], row[\"output\"])\n",
    "    prompt_ids = tok(prompt, add_special_tokens=False).input_ids\n",
    "    enc = tok(prompt + answer, add_special_tokens=False,\n",
    "              max_length=BLOCK, padding=\"max_length\", truncation=True)\n",
    "    labels = enc[\"input_ids\"].copy()\n",
    "    pl = min(len(prompt_ids), BLOCK)\n",
    "    for i in range(pl): labels[i] = -100\n",
    "    return {\n",
    "        \"input_ids\": torch.tensor(enc[\"input_ids\"], dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(enc[\"attention_mask\"], dtype=torch.long),\n",
    "        \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "    }\n",
    "\n",
    "train_rows = [encode_row_fixed(r) for r in train]\n",
    "print(\"1 sample lens:\", len(train_rows[0][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f1a25d4-d569-4eca-92ad-7786a579b7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': torch.Size([8, 256]), 'attention_mask': torch.Size([8, 256]), 'labels': torch.Size([8, 256])}\n"
     ]
    }
   ],
   "source": [
    "#E) DataLoader\n",
    "class ListDS(Dataset):\n",
    "    def __init__(self, items): self.items=items\n",
    "    def __len__(self): return len(self.items)\n",
    "    def __getitem__(self,i): return self.items[i]\n",
    "#[\n",
    "#  {\"input_ids\": tensor([...]), \"attention_mask\": tensor([...]), \"labels\": tensor([...])},\n",
    "#  {\"input_ids\": tensor([...]), \"attention_mask\": tensor([...]), \"labels\": tensor([...])},\n",
    "#  ...\n",
    "#]\n",
    "def collate(batch):\n",
    "    keys = batch[0].keys()\n",
    "    out={}\n",
    "    for k in keys:\n",
    "        out[k]=torch.stack([b[k] for b in batch], dim=0)\n",
    "    return out\n",
    "#{\n",
    "#  \"input_ids\": tensor([[...], [...], ...]),         # shape: (batch_size, seq_len)\n",
    "#  \"attention_mask\": tensor([[...], [...], ...]),\n",
    "#  \"labels\": tensor([[...], [...], ...])\n",
    "#}\n",
    "loader = DataLoader(ListDS(train_rows), batch_size=BATCH, shuffle=True, collate_fn=collate)\n",
    "batch = next(iter(loader))\n",
    "print({ k: v.shape for k,v in batch.items() })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "205af190-96bf-4cc1-a1f3-1bc8553fb389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#F) ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆloss/å­¦ç¿’/ç”Ÿæˆ/è©•ä¾¡ãƒ»chrFä»£æ›¿ï¼‰\n",
    "from collections import Counter\n",
    "\n",
    "def avg_loss(model, loader, n_batches=6):\n",
    "    model.eval(); it=iter(loader); losses=[]\n",
    "    with torch.no_grad():\n",
    "        for _ in range(n_batches):\n",
    "            try: b=next(it)\n",
    "            except StopIteration: break\n",
    "            b={k:v.to(DEVICE) for k,v in b.items()}\n",
    "            losses.append(model(**b).loss.item())\n",
    "    return float(np.mean(losses)) if losses else float(\"nan\")\n",
    "\n",
    "def train_steps(model, loader, lr=3e-4, steps=150):\n",
    "    opt = torch.optim.AdamW([p for p in model.parameters() if p.requires_grad], lr=lr)\n",
    "    model.train(); it=iter(loader)\n",
    "    tok_count=0; t0=time.time()\n",
    "    for s in range(steps):\n",
    "        try: b=next(it)\n",
    "        except StopIteration: it=iter(loader); b=next(it)\n",
    "        b={k:v.to(DEVICE) for k,v in b.items()}\n",
    "        opt.zero_grad(); loss=model(**b).loss; loss.backward(); opt.step()\n",
    "        tok_count += int(b[\"attention_mask\"].sum().item())\n",
    "        if (s+1)%50==0: print(f\"step {s+1}/{steps} loss={loss.item():.4f}\")\n",
    "        if DEVICE==\"mps\" and (s+1)%25==0:\n",
    "            try: torch.mps.empty_cache()\n",
    "            except: pass\n",
    "    tps = tok_count / max(time.time()-t0, 1e-6)\n",
    "    return model, int(tps)\n",
    "\n",
    "def generate_answers(model, prompts, max_new_tokens=40):\n",
    "    model.eval(); outs=[]\n",
    "    for p in prompts:\n",
    "        enc = tok(p, return_tensors=\"pt\")\n",
    "        enc = {k:v.to(DEVICE) for k,v in enc.items()}\n",
    "        with torch.no_grad():\n",
    "            gen = model.generate(**enc, max_new_tokens=max_new_tokens, do_sample=False,\n",
    "                                 eos_token_id=tok.eos_token_id, pad_token_id=tok.pad_token_id)\n",
    "        text = tok.decode(gen[0][enc[\"input_ids\"].shape[1]:], skip_special_tokens=True).strip()\n",
    "        outs.append(text)\n",
    "    return outs\n",
    "\n",
    "# evaluateç„¡ã—ã®å®‰å®šã‚¹ã‚³ã‚¢ï¼ˆæ–‡å­—å˜ä½F1ï¼‰\n",
    "def char_f1(pred, ref):\n",
    "    p=list(pred); r=list(ref)\n",
    "    if len(p)==0 or len(r)==0: return 0.0\n",
    "    cp, cr = Counter(p), Counter(r)\n",
    "    overlap = sum((cp & cr).values())\n",
    "    prec = overlap/len(p); rec = overlap/len(r)\n",
    "    return 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n",
    "\n",
    "def safe_chrf(preds, refs):\n",
    "    preds=[(\"\" if p is None else str(p).strip()) for p in preds]\n",
    "    refs =[(\"\" if r is None else str(r).strip()) for r in refs]\n",
    "    if not preds or not refs: return 0.0\n",
    "    scores=[char_f1(p,r) for p,r in zip(preds,refs)]\n",
    "    return float(np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5613306-0d02-42dd-ab91-94261b0b07f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hold size: 200\n"
     ]
    }
   ],
   "source": [
    "#G) ãƒ›ãƒ¼ãƒ«ãƒ‰ã‚¢ã‚¦ãƒˆä½œæˆ\n",
    "hold_prompts = [apply_template(r[\"instruction\"])[0] for r in hold]\n",
    "hold_refs    = [r[\"output\"] for r in hold]\n",
    "print(\"hold size:\", len(hold_prompts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c587274-c0e9-4a69-8c63-b05e11923826",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==> A: LoRA\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mh/opt/anaconda3/envs/ai-train-2025/lib/python3.11/site-packages/peft/tuners/lora/layer.py:2174: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n",
      "  warnings.warn(\n",
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 50/150 loss=0.4391\n",
      "step 100/150 loss=1.2159\n",
      "step 150/150 loss=0.7185\n",
      "\n",
      "==> B: Head-only\n",
      "step 50/150 loss=1.3129\n",
      "step 100/150 loss=2.2216\n",
      "step 150/150 loss=1.4937\n",
      "\n",
      "Summary: {'LoRA': {'loss': (9.5016, 1.1357), 'chrF': 0.3885, 'tok/s': 4102}, 'Head': {'loss': (7.6471, 1.0598), 'chrF': 0.0026, 'tok/s': 3969}}\n"
     ]
    }
   ],
   "source": [
    "#H) å®Ÿè¡Œï¼šA=LoRA / B=Head-only\n",
    "def fresh_base():\n",
    "    return AutoModelForCausalLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "\n",
    "# A) LoRA\n",
    "print(\"\\n==> A: LoRA\")\n",
    "base_A = fresh_base()\n",
    "lcfg = LoraConfig(r=8, lora_alpha=16, lora_dropout=0.05, bias=\"none\",\n",
    "                  target_modules=[\"c_attn\",\"c_proj\"], task_type=\"CAUSAL_LM\")\n",
    "modelA = get_peft_model(base_A, lcfg)\n",
    "b0A = avg_loss(modelA, loader)\n",
    "modelA, tpsA = train_steps(modelA, loader, lr=3e-4, steps=150)\n",
    "b1A = avg_loss(modelA, loader)\n",
    "predA = generate_answers(modelA, hold_prompts)\n",
    "cA   = safe_chrf(predA, hold_refs)\n",
    "\n",
    "# B) Head-onlyï¼ˆLMãƒ˜ãƒƒãƒ‰ã®ã¿å­¦ç¿’ï¼‰\n",
    "print(\"\\n==> B: Head-only\")\n",
    "base_B = fresh_base()\n",
    "for p in base_B.parameters(): p.requires_grad=False\n",
    "base_B.lm_head.weight.requires_grad=True\n",
    "b0B = avg_loss(base_B, loader)\n",
    "base_B, tpsB = train_steps(base_B, loader, lr=5e-4, steps=150)\n",
    "b1B = avg_loss(base_B, loader)\n",
    "predB = generate_answers(base_B, hold_prompts)\n",
    "cB   = safe_chrf(predB, hold_refs)\n",
    "\n",
    "print(\"\\nSummary:\",\n",
    "      {\"LoRA\":   {\"loss\": (round(b0A,4), round(b1A,4)), \"chrF\": round(cA,4), \"tok/s\": tpsA},\n",
    "       \"Head\":   {\"loss\": (round(b0B,4), round(b1B,4)), \"chrF\": round(cB,4), \"tok/s\": tpsB}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99847717-e812-4f37-bf12-6dc1eb58bd91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved: results/hf_lora_mps.tsv , results/notes.txt\n"
     ]
    }
   ],
   "source": [
    "#I) ä¿å­˜ï¼ˆTSV / notesï¼‰\n",
    "os.makedirs(\"results\", exist_ok=True)\n",
    "out_tsv = \"results/hf_lora_mps.tsv\"\n",
    "with open(out_tsv,\"w\",newline=\"\") as f:\n",
    "    w=csv.writer(f, delimiter=\"\\t\")\n",
    "    w.writerow([\"dataset\",\"model\",\"variant\",\"loss_before\",\"loss_after\",\"chrF\",\"tokens_per_sec\"])\n",
    "    w.writerow([DATASET, MODEL_NAME, \"LoRA\",  round(b0A,4), round(b1A,4), round(cA,4), tpsA])\n",
    "    w.writerow([DATASET, MODEL_NAME, \"Head\",  round(b0B,4), round(b1B,4), round(cB,4), tpsB])\n",
    "\n",
    "notes = \"results/notes.txt\"\n",
    "with open(notes,\"a\") as f:\n",
    "    ts=time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    f.write(f\"[{ts}] DATASET={DATASET} MODEL={MODEL_NAME} BLOCK={BLOCK} BATCH={BATCH} DEVICE={DEVICE}\\n\")\n",
    "    f.write(f\" LoRA : loss {b0A:.4f}->{b1A:.4f} | chrF={cA:.4f} | tok/s={tpsA}\\n\")\n",
    "    f.write(f\" Head : loss {b0B:.4f}->{b1B:.4f} | chrF={cB:.4f} | tok/s={tpsB}\\n\")\n",
    "    f.write(\"\\n--- Samples (first 2) ---\\n\")\n",
    "    for i in range(min(2, len(hold_prompts))):\n",
    "        f.write(f\"PROMPT:\\n{hold_prompts[i]}\\nREF : {hold_refs[i]}\\n\\n\")\n",
    "print(\"saved:\", out_tsv, \",\", notes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8c52de-f148-4414-bdf5-83e91895984e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ai-train-2025)",
   "language": "python",
   "name": "ai-train-2025"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
